{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d6cb3e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'model_state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m model \u001b[38;5;241m=\u001b[39m NeuroSegNet(config)\n\u001b[0;32m     17\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexperiments/neurosegnet_v1/checkpoints/best_model.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_state_dict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     19\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     20\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'model_state_dict'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append(\"C:/Users/user/NeuroSegNet/\")\n",
    "from monai.transforms import LoadImage, Resize\n",
    "from src.xai.gradcam import apply_gradcam\n",
    "from src.models.neurosegnet import NeuroSegNet\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# Load config and model\n",
    "with open(\"C:/Users/user/NeuroSegNet/configs/model/neurosegnet.yml\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "model = NeuroSegNet(config)\n",
    "checkpoint = torch.load(\"experiments/neurosegnet_v1/checkpoints/best_model.pth\", map_location=\"cpu\")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e2fdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a validation case\n",
    "image_path = sorted(Path(\"data/val_preprocessed/\").glob(\"*.nii.gz\"))[0]\n",
    "loader = LoadImage(image_only=True)\n",
    "img_np = loader(str(image_path))  # [H,W,D,C]\n",
    "img_np = np.moveaxis(img_np, -1, 0)  # [C,H,W,D]\n",
    "input_tensor = torch.tensor(img_np, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "# Apply GradCAM\n",
    "target_layer = model.encoder.layers[-1]\n",
    "cam = apply_gradcam(model, input_tensor, target_layer, resize_to=(128,128,128))\n",
    "\n",
    "# Visualize mid-slice\n",
    "plt.figure(figsize=(8,4))\n",
    "mid_slice = cam.shape[2] // 2\n",
    "plt.imshow(cam[:, :, mid_slice], cmap='hot')\n",
    "plt.title(\"Grad-CAM (Mid Slice)\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brain_mri_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
